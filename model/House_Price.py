# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HdLqi1EEF_kfHaPaUVmDMuuuuCfJ4faU
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
# %matplotlib inline
import matplotlib
matplotlib.rcParams["figure.figsize"]=(20,10)


def is_not_float(x):
  try:
    float(x)
  except:
    return True
  return False

def convert_to_sqft(x):
  ele=x.split(' - ')
  if len(ele)==2:
    return (float(ele[0])+float(ele[1]))/2
  try:
    return float(x)
  except:
    return None

def plot_scatter_chart(df, location):
  bhk2=df[(df['location']==location) & (df['size']==2)]
  bhk3=df[(df['location']==location) & (df['size']==3)]
  matplotlib.rcParams["figure.figsize"]=(15,10)
  plt.scatter(bhk2.total_sqft, bhk2.price_per_sqft, color='blue', label='2 BHK', s=50)
  plt.scatter(bhk3.total_sqft, bhk3.price_per_sqft, marker='+', color='green', label='3 BHK', s=50)
  plt.xlabel("Total SQFT")
  plt.ylabel("Price per sqft")
  plt.title(location)
  plt.legend()

def remove_bed_size_outliers(df):
  return df['total_sqft_per_BHK']<300

def remove_pps_outliers(df):
  df_out=pd.DataFrame()
  for key, subdf in df.groupby('location'):
    m=np.mean(subdf.price_per_sqft)
    st=np.std(subdf.price_per_sqft)
    reduced_df=subdf[(subdf.price_per_sqft>(m-st)) & (subdf.price_per_sqft<=(m+st))]
    df_out=pd.concat([df_out,reduced_df], ignore_index=True)
  return df_out

def remove_bhk_outliers(df):
  exclude_indices=np.array([])
  for location, location_df in df.groupby('location'):
    bhk_stats={}
    for bhk, bhk_df in location_df.groupby('size'):
      bhk_stats[bhk]={
          'mean': np.mean(bhk_df['price_per_sqft']),
          'std': np.std(bhk_df['price_per_sqft']),
          'count': bhk_df.shape[0]
      }

    for bhk, bhk_df in location_df.groupby('size'):
      stats=bhk_stats.get(bhk-1)
      if stats and  stats['count']>5:
        exclude_indices=np.append(exclude_indices, bhk_df[bhk_df['price_per_sqft']<(stats['mean'])].index.values)
    return df.drop(exclude_indices, axis='index')

df=pd.read_csv("Bengaluru_House_Data.csv")
df.groupby('area_type')['area_type'].agg('count')
df.drop(['area_type','availability','society'],axis='columns',inplace=True)
new_df=df.copy()

new_df.isnull().sum()

new_df['balcony']=new_df['balcony'].fillna(0)
new_df['bath']=new_df['bath'].fillna(new_df['bath'].mean())
new_df=new_df.dropna()
new_df['size']=new_df['size'].apply(lambda x: int(x.split(' ')[0]))



new_df[(new_df['total_sqft']).apply(lambda x: is_not_float(x))].total_sqft.unique()
new_df['total_sqft']=new_df['total_sqft'].apply(lambda x: convert_to_sqft(x))
new_df['total_sqft']=new_df['total_sqft'].fillna(new_df['total_sqft'].mean())
new_df['price_per_sqft']=new_df['price']*100000/new_df['total_sqft']
new_df['location']=new_df['location'].apply(lambda x: x.strip())
location_stats=new_df.groupby('location')['location'].agg('count').sort_values(ascending=False)
new_df['location']=new_df['location'].apply(lambda x: 'others' if x in location_stats[location_stats<=10] else x)
new_df['total_sqft_per_BHK']=new_df['total_sqft']/new_df['size']
new_df=new_df[~(remove_bed_size_outliers(new_df))]
new_df=remove_pps_outliers(new_df)
new_df=remove_bhk_outliers(new_df)
new_df=new_df[new_df['bath']<=new_df['size']+2]
new_df.drop(['price_per_sqft','total_sqft_per_BHK'],axis='columns')

dummies=pd.get_dummies(new_df['location'])
dummies.drop('others',axis='columns')
new_df=pd.concat([new_df,dummies], axis='columns')
new_df=new_df.drop('location', axis='columns')

X=new_df.drop(['price','price_per_sqft', 'total_sqft_per_BHK'], axis='columns')
y=new_df['price']

from sklearn.model_selection import train_test_split
X_train,X_test, y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=10)

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.tree import DecisionTreeRegressor

def find_best_model(X,y):
  algos={
      'lasso':{
          'model': Lasso(),
          'params': {
              'alpha':[1,2],
              'selection':['random','cyclic']
            }
      },
      'linear_regression':{
          'model': LinearRegression(),
          'params': {
              'normalize':[True, False]
          }
      },
      'decision_tree':{
            'model': DecisionTreeRegressor(),
            'params':{
                'criterion':['mse','friedman_mse'],
                'splitter':['best','random']
              }
      }
  }

  scores=[]
  cv=ShuffleSplit(n_splits=5, test_size=0.2,random_state=0)
  for algo_name, config in algos.items():
    gs=GridSearchCV(config['model'],config['params'],cv=cv, return_train_score=False)
    gs.fit(X,y)
    scores.append({
        'model':algo_name,
        'best_score':gs.best_score_,
        'best_params':gs.best_params_
    })
  return pd.DataFrame(scores, columns=['model','best_score','best_params'])

find_best_model(X,y)

reg=LinearRegression()
reg.fit(X_train,y_train)

def predict_price(location, size, sqft, bath, balcony):
  loc_index=np.where(X.columns==location)[0][0]

  x=np.zeros(len(X.columns))
  x[0]=size
  x[1]=sqft
  x[2]=bath
  x[3]=balcony

  if loc_index>=0:
    x[loc_index]=1

  return reg.predict([x])[0]

import pickle
import json
with open('Bangalore_House_Data_Model.pickle','wb') as fp:
  pickle.dump(reg, fp)
columns={
    'data_columns':[col.lower() for col in X.columns]
}
with open('columns.json','w') as fc:
  fc.write(json.dumps(columns))
